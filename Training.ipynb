{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, GRU, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'your_path_here.csv' #https://www.kaggle.com/datasets/andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews\n",
    "\n",
    "data_set = pd.read_csv(path)\n",
    "\n",
    "y= data_set.iloc[:,0].values\n",
    "X = data_set.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "(X_train_text,X_test_text,y_train,y_test) = train_test_split(X,y, test_size = 0.3, random_state =  2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 10000)\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token  = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_string = tokenizer.word_index\n",
    "Inverse_map = dict(zip(token_string.values(), token_string.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_text(tokens):\n",
    "    strings = [Inverse_map[token] for token in tokens if token != 0]\n",
    "    text = \" \".join(strings)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_max = 241\n",
    "#Padding the sequence \n",
    "X_train_pad = pad_sequences(X_train_token, maxlen = len_max, padding = 'pre')\n",
    "X_test_pad = pad_sequences(X_test_token, maxlen = len_max, padding ='pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0   16    2 1124   41   12    2 1124  166    5 1291   74\n",
      " 5131   74 3920]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embedding_Layer (Embedding)  (None, 241, 20)           400000    \n",
      "_________________________________________________________________\n",
      "First_Layer (GRU)            (None, 241, 16)           1824      \n",
      "_________________________________________________________________\n",
      "Second_layer (GRU)           (None, 241, 8)            624       \n",
      "_________________________________________________________________\n",
      "Third_Layer (GRU)            (None, 4)                 168       \n",
      "_________________________________________________________________\n",
      "Dense_Layer (Dense)          (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 402,621\n",
      "Trainable params: 402,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "embedding_size = 20\n",
    "Model = Sequential()\n",
    "Model.add(Embedding(input_dim = 20000, input_length = len_max, output_dim = embedding_size, name = 'Embedding_Layer'))\n",
    "Model.add(GRU(units = 16, return_sequences = True, name='First_Layer'))\n",
    "Model.add(GRU(units = 8, return_sequences = True, name ='Second_layer'))\n",
    "Model.add(GRU(units = 4, name = \"Third_Layer\"))\n",
    "Model.add(Dense(1, activation = 'sigmoid', name =\"Dense_Layer\"))\n",
    "optimizer = Adam(lr = 0.001)\n",
    "Model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'], optimizer = optimizer)\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9975/9975 [==============================] - 2036s 204ms/step - loss: 0.4304 - accuracy: 0.7986 - val_loss: 0.3871 - val_accuracy: 0.8254\n",
      "Epoch 2/5\n",
      "9975/9975 [==============================] - 18231s 2s/step - loss: 0.3529 - accuracy: 0.8456 - val_loss: 0.3680 - val_accuracy: 0.8379\n",
      "Epoch 3/5\n",
      "9975/9975 [==============================] - 106802s 11s/step - loss: 0.3145 - accuracy: 0.8665 - val_loss: 0.3570 - val_accuracy: 0.8457\n",
      "Epoch 4/5\n",
      "9975/9975 [==============================] - 2067s 207ms/step - loss: 0.2820 - accuracy: 0.8832 - val_loss: 0.3623 - val_accuracy: 0.8471\n",
      "Epoch 5/5\n",
      "9975/9975 [==============================] - 24245s 2s/step - loss: 0.2535 - accuracy: 0.8978 - val_loss: 0.3602 - val_accuracy: 0.8537\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embedding_Layer (Embedding)  (None, 241, 20)           400000    \n",
      "_________________________________________________________________\n",
      "First_Layer (GRU)            (None, 241, 16)           1824      \n",
      "_________________________________________________________________\n",
      "Second_layer (GRU)           (None, 241, 8)            624       \n",
      "_________________________________________________________________\n",
      "Third_Layer (GRU)            (None, 4)                 168       \n",
      "_________________________________________________________________\n",
      "Dense_Layer (Dense)          (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 402,621\n",
      "Trainable params: 402,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Fitting the model\n",
    "Model.fit(X_train_pad, y_train, validation_split = 0.05,epochs =5, batch_size = 32)\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10500/10500 [==============================] - 604s 58ms/step - loss: 0.2164 - accuracy: 0.9187\n",
      "4500/4500 [==============================] - 252s 56ms/step - loss: 0.3533 - accuracy: 0.8545\n",
      "The accuracy on the training set is 91.87%\n",
      "The accuracy on the test set is 85.45%\n"
     ]
    }
   ],
   "source": [
    "result = Model.evaluate(X_train_pad,y_train)\n",
    "Evaluate = Model.evaluate(X_test_pad,y_test)\n",
    "print(\"The accuracy on the training set is {0:.2%}\".format(result[1]))\n",
    "print(\"The accuracy on the test set is {0:.2%}\".format(Evaluate[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(text):\n",
    "    text_seq = tokenizer.texts_to_sequences(text)\n",
    "    #text_seq = [np.array(a) for a in text_seq]\n",
    "    text_token = pad_sequences(text_seq, maxlen = len_max, padding = 'pre')\n",
    "    pred = Model.predict(text_token)\n",
    "    if pred[0] >= 0.5:\n",
    "        result = 'Positive Review! Thank you'\n",
    "    else:\n",
    "        result = \"Negative Review! We will improve our delivery\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Review! We will improve our delivery\n"
     ]
    }
   ],
   "source": [
    "text = [\"I  love the movie\"]\n",
    "print(classifier(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Model\n",
    "Model.save(\"model/Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('tokenize.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
